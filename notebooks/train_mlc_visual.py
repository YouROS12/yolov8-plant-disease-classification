# -*- coding: utf-8 -*-
"""Train_MLC_Visual.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11iXfxymZ_V4-OGN0NpfxpoEn8dYsW6_b
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import os
import time
import joblib
import matplotlib.pyplot as plt
import seaborn as sns


from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score,classification_report

# --- Import Classifiers ---
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import lightgbm as lgb
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier

# --- CHOOSE WHICH EXPERIMENT TO RUN ---
EXPERIMENT_NAME = "visual_grouping" # CHANGE THIS FOR EACH EXPERIMENT

# --- Base Directory for All New Results ---
DRIVE_RESULTS_PATH = '/content/drive/MyDrive/PlantWild_Final_Tuned_Experiments/'
os.makedirs(DRIVE_RESULTS_PATH, exist_ok=True)

# --- Data Loading Logic ---
print(f"--- Loading data for experiment: {EXPERIMENT_NAME} ---")
data_paths = {
    "treatment_grouping": '/content/drive/MyDrive/MyPlantDisease_PCA_Data/',
    "visual_grouping": '/content/drive/MyDrive/MyPlantDisease_IncrementalPCA_Data_visual'
}
load_path = data_paths.get(EXPERIMENT_NAME)

if load_path is None or not os.path.exists(load_path):
    raise FileNotFoundError(f"Data path for experiment '{EXPERIMENT_NAME}' not found at '{load_path}'.")

x_train_pool_pca = np.load(os.path.join(load_path, 'x_train_pool_pca.npy'))
y_mlc_train_pool_encoded = np.load(os.path.join(load_path, 'y_mlc_train_pool_encoded.npy'))
x_final_test_pca = np.load(os.path.join(load_path, 'x_final_test_pca.npy'))
y_mlc_final_test_encoded = np.load(os.path.join(load_path, 'y_mlc_final_test_encoded.npy'))

x_train, x_val, y_train, y_val = train_test_split(
    x_train_pool_pca, y_mlc_train_pool_encoded,
    test_size=0.2, stratify=y_mlc_train_pool_encoded,
    shuffle=True, random_state=42
)
x_test = x_final_test_pca
y_test = y_mlc_final_test_encoded

print("Data loaded and splits recreated successfully.")
print(f"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}")
print(f"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}")

# --- Setup Save Directories ---
experiment_path = os.path.join(DRIVE_RESULTS_PATH, EXPERIMENT_NAME)
cm_path = os.path.join(experiment_path, 'confusion_matrices')
plots_path = os.path.join(experiment_path, 'plots')
os.makedirs(cm_path, exist_ok=True)
os.makedirs(plots_path, exist_ok=True)

print(f"\nResults for this run will be saved in: {experiment_path}")

all_results = []

from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from cuml.svm import SVC as cuSVC
from cuml.neighbors import KNeighborsClassifier as cuKNN

models_to_run = {
    "XGBoost": {
        "estimator": XGBClassifier(
            random_state=42,
            use_label_encoder=False,
            eval_metric='mlogloss',
            tree_method='gpu_hist'
        ),
        "params": {
            'n_estimators': [100, 300],
            'max_depth': [3, 6, 9],
            'learning_rate': [0.01, 0.1],
            'subsample': [0.8, 1.0]
        }
    },
    "RandomForest": {
        "estimator": RandomForestClassifier(random_state=42, n_jobs=-1),
        "params": {
            'n_estimators': [100, 300],
            'max_depth': [None, 20],
            'min_samples_split': [2, 5]
        }
    },
    "MLPClassifier": {
        "estimator": MLPClassifier(random_state=42, max_iter=500, early_stopping=True),
        "params": {
            'hidden_layer_sizes': [(100,), (200,)],
            'alpha': [0.0001, 0.001],
            'learning_rate_init': [0.001, 0.01]
        }
    },

    "SVM": {
    "estimator": SVC(probability=True, random_state=42),
    "params": {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto']}
    }
}

print(f"Models to run for this experiment: {list(models_to_run.keys())}")

# Path for the summary CSV file, defined once before the loop
summary_csv_path = os.path.join(experiment_path, 'metrics_summary.csv')

for model_name, model_info in models_to_run.items():
    print(f"\n{'='*30}\nStarting process for: {model_name}\n{'='*30}")

    start_time_tuning = time.time()
    grid_search = GridSearchCV(
        estimator=model_info["estimator"],
        param_grid=model_info["params"],
        cv=2,
        scoring='f1_macro',
        verbose=1,
        n_jobs=-1
    )

    x_train_full = np.vstack((x_train, x_val))
    y_train_full = np.hstack((y_train, y_val))
    grid_search.fit(x_train_full, y_train_full)
    end_time_tuning = time.time()
    tuning_time = end_time_tuning - start_time_tuning

    print(f"\nBest parameters for {model_name}: {grid_search.best_params_}")
    print(f"Best CV F1 (macro) score: {grid_search.best_score_:.4f}")

    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(x_test)

    test_accuracy = accuracy_score(y_test, y_pred)
    test_f1_macro = f1_score(y_test, y_pred, average='macro')

    print(f"Test Accuracy: {test_accuracy:.4f}")
    print(f"Test F1 (macro): {test_f1_macro:.4f}")

    result_entry = {
        'Labeling_Strategy': EXPERIMENT_NAME,
        'Classifier': model_name,
        'Accuracy': test_accuracy,
        'F1_Macro': test_f1_macro,
        'Time_s': tuning_time,
        'Best_Params': str(grid_search.best_params_)
    }
    all_results.append(result_entry)

    # --- INCREMENTAL SAVE ---
    # Convert the entire list of results so far to a DataFrame and save it.
    # This overwrites the file each time, ensuring it's always up to date.
    summary_df = pd.DataFrame(all_results)
    summary_df.to_csv(summary_csv_path, index=False)
    print(f"Metrics summary updated and saved to {summary_csv_path}")

    # Save the confusion matrix plot
    cm = confusion_matrix(y_test, y_pred)
    num_classes = len(np.unique(np.concatenate((y_test, y_pred))))
    figsize = max(10, num_classes // 1.5)
    plt.figure(figsize=(figsize, figsize-2))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix for {model_name} on {EXPERIMENT_NAME}')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    cm_filename = os.path.join(cm_path, f'{model_name}_cm.png')
    plt.savefig(cm_filename, bbox_inches='tight')
    plt.close()
    print(f"Confusion matrix saved to {cm_filename}")

    # Save the best model
    model_filename = os.path.join(experiment_path, f'{model_name}_best_model.joblib')
    joblib.dump(best_model, model_filename)
    print(f"Best model saved to {model_filename}")

print(f"\n{'='*30}\nExperiment '{EXPERIMENT_NAME}' complete. Full summary saved to {summary_csv_path}")

"""**SVC (Advanced Tuning)**"""

import pandas as pd
import numpy as np
import os
import time
import joblib
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score

# --- 1. Configuration and Data Loading ---

# CHOOSE WHICH EXPERIMENT TO RUN
# This variable controls which saved data is loaded from your Drive.
# Options: "treatment_grouping", "visual_grouping"
EXPERIMENT_NAME = "treatment_grouping"

# Define paths for loading data and saving results
DRIVE_RESULTS_PATH = '/content/drive/MyDrive/PlantWild_Final_Tuned_Experiments/'
DATA_LOAD_PATH = f'/content/drive/MyDrive/MyPlantDisease_PCA_Data/'

# Create the save directory for this specific run
SAVE_PATH = os.path.join(DRIVE_RESULTS_PATH, EXPERIMENT_NAME)
os.makedirs(SAVE_PATH, exist_ok=True)

print(f"--- Starting Self-Contained SVC Tuning for Experiment: '{EXPERIMENT_NAME}' ---")
print(f"Loading data from: {DATA_LOAD_PATH}")
print(f"Results will be saved in: {SAVE_PATH}")

try:
    # Load the four key numpy arrays
    x_train_pool_pca = np.load(os.path.join(DATA_LOAD_PATH, 'x_train_pool_pca.npy'))
    y_mlc_train_pool_encoded = np.load(os.path.join(DATA_LOAD_PATH, 'y_mlc_train_pool_encoded.npy'))
    x_final_test_pca = np.load(os.path.join(DATA_LOAD_PATH, 'x_final_test_pca.npy'))
    y_mlc_final_test_encoded = np.load(os.path.join(DATA_LOAD_PATH, 'y_mlc_final_test_encoded.npy'))
    print("Data loaded successfully.")
except FileNotFoundError as e:
    raise FileNotFoundError(f"Data file not found. Please check the DATA_LOAD_PATH. Error: {e}")


# --- 2. Data Structuring: Create Final Train and Test Splits ---

# We create the train/test splits here directly from the loaded pool files.
# The 'test' set is the final, held-out data.
# The 'train' set will be used for cross-validated tuning.
x_train = x_train_pool_pca
y_train = y_mlc_train_pool_encoded
x_test = x_final_test_pca
y_test = y_mlc_final_test_encoded

print("\nData structured successfully:")
print(f"x_train shape (for CV tuning): {x_train.shape}, y_train shape: {y_train.shape}")
print(f"x_test shape (for final evaluation): {x_test.shape}, y_test shape: {y_test.shape}")


# --- 3. Hyperparameter Tuning ---

model_name_file = "SVC_Advanced_Tuned"
model_name_display = "SVC (Advanced Tuning)"

print(f"\n{'='*30}\nStarting {model_name_display}\n{'='*30}")

try:
    pipeline = Pipeline([
        ('classifier', SVC(probability=True, random_state=42))
    ])

    param_grid = {
        'classifier__C': [1, 10, 50, 100],
        'classifier__gamma': ['scale', 'auto', 0.001, 0.01],
        'classifier__kernel': ['rbf'],
        'classifier__class_weight': ['balanced', None]
    }

    grid_search = GridSearchCV(
        pipeline,
        param_grid,
        cv=3,
        scoring='f1_macro',
        verbose=2,
        n_jobs=-1
    )

    print("\nFitting GridSearchCV for SVC on the training set...")
    start_time_tuning = time.time()

    # Fit the Grid Search ONLY on the training data.
    grid_search.fit(x_train, y_train)

    end_time_tuning = time.time()
    tuning_time = end_time_tuning - start_time_tuning

    print("\n--- Tuning Complete ---")
    print(f"Total tuning time: {tuning_time:.2f} seconds")
    print("\nBest parameters found: ", grid_search.best_params_)
    print("Best cross-validation F1 Macro score on the training set: {:.4f}".format(grid_search.best_score_))

    best_model_svc_pipeline = grid_search.best_estimator_

    # --- 4. Final Evaluation and Saving ---

    print("\n--- Evaluating Best Tuned SVC Model on the Held-Out Test Set ---")
    y_pred_test = best_model_svc_pipeline.predict(x_test)

    report = classification_report(y_test, y_pred_test, output_dict=True)
    report_df = pd.DataFrame(report).transpose()

    print("Test Set Classification Report:")
    print(report_df)

    test_accuracy = report['accuracy'] * 100
    test_f1_macro = report['macro avg']['f1-score']

    final_metrics_dict = {
        'model_name': model_name_display,
        'test_accuracy': test_accuracy,
        'test_f1_macro': test_f1_macro,
        'test_precision_macro': report['macro avg']['precision'],
        'test_recall_macro': report['macro avg']['recall'],
        'tuning_time_s': tuning_time,
        'best_cv_score_f1_macro': grid_search.best_score_,
        'best_params': str(grid_search.best_params_)
    }

    pd.DataFrame([final_metrics_dict]).to_csv(os.path.join(SAVE_PATH, f'{model_name_file}_metrics.csv'), index=False)
    print(f"\nSuccessfully saved tuned metrics for {model_name_display}.")

    joblib.dump(best_model_svc_pipeline, os.path.join(SAVE_PATH, f'{model_name_file}_model.joblib'))
    print(f"Successfully saved the best tuned model for {model_name_display}.")

except Exception as e:
    print(f"An error occurred during the SVC tuning process: {e}")

"""**Step 2.1: Generating Per-Class Performance Metrics (Table)**"""

#
# CELL 2: PER-CLASS PERFORMANCE METRICS
#
print(f"--- Step 2.1: Generating Per-Class Metrics for 'visual_grouping' ---")

# --- Configuration ---
EXPERIMENT_NAME = "visual_grouping"

# Define paths based on your setup
DRIVE_RESULTS_PATH = "/content/drive/MyDrive/PlantWild_Final_Tuned_Experiments"
LOAD_PATH = os.path.join(DRIVE_RESULTS_PATH, EXPERIMENT_NAME)
MODEL_NAME = "SVM_best_model"
MODEL_PATH = os.path.join(LOAD_PATH, f"{MODEL_NAME}.joblib")
REPORT_SAVE_PATH = os.path.join(LOAD_PATH, f"{MODEL_NAME}_per_class_report.csv")

# --- Load the saved model ---
try:
    print(f"Loading best SVM model from: {MODEL_PATH}")
    best_svc_model = joblib.load(MODEL_PATH)
except FileNotFoundError:
    print(f"ERROR: Model not found at {MODEL_PATH}.")
    print("Please ensure you have run the SVM training cell for this experiment first.")
    raise

# --- Generate Predictions and Report ---
print("Evaluating model on the final test set...")
y_pred_test = best_svc_model.predict(x_test)

# --- Define visual class names ---
visual_categories = [
    "rust",
    "powdery_mildew",
    "downy_mildew",
    "blight",
    "leaf_spot",
    "virus",
    "rot",
    "anthracnose",
    "scab",
    "bacterial_disease",
    "mold",
    "septoria_blotch",
    "smut",
    "leaf_scorch",
    "fungal_gall",
    "mummy_berry",
    "fungal_wilt",
    "oomycete_lesion",
    "berry_blotch"
]

# Align unique labels with the visual categories
unique_labels = np.unique(y_test)
target_names = [visual_categories[i] for i in unique_labels]

# --- Generate Classification Report ---
report = classification_report(y_test, y_pred_test, target_names=target_names, output_dict=True)

# Convert to DataFrame and format
report_df = pd.DataFrame(report).transpose().round(4)

# --- Display and Save ---
print("\n--- Per-Class Performance Report ---")
print(report_df)

report_df.to_csv(REPORT_SAVE_PATH)
print(f"\nSuccessfully saved per-class report to: {REPORT_SAVE_PATH}")
print("\nACTION: Copy this table into your paper's supplementary material or results section.")

"""**Step 2.2: Confidence Calibration Analysis (Plot & Metric)**"""

#
# CELL 3: CONFIDENCE CALIBRATION ANALYSIS (ECE and Reliability Diagram)
#
from sklearn.calibration import calibration_curve
import matplotlib.pyplot as plt # Import matplotlib

print(f"--- Step 2.2: Running Confidence Calibration for '{EXPERIMENT_NAME}' ---")

# --- Configuration ---
LOAD_PATH = os.path.join(DRIVE_RESULTS_PATH, EXPERIMENT_NAME)
MODEL_NAME = "SVM_best_model"
MODEL_PATH = os.path.join(LOAD_PATH, f'{MODEL_NAME}.joblib')
PLOT_SAVE_PATH = os.path.join(LOAD_PATH, f'{MODEL_NAME}_reliability_diagram.png')
METRICS_SAVE_PATH = os.path.join(LOAD_PATH, f'{MODEL_NAME}_calibration_metrics.csv')

# --- Load the Model ---
print(f"Loading best tuned SVC model from: {MODEL_PATH}")
best_svc_model = joblib.load(MODEL_PATH)

# Check if the model can predict probabilities
if not hasattr(best_svc_model, "predict_proba"):
    raise TypeError("The loaded model cannot predict probabilities. Please ensure it was trained with `probability=True`.")

# --- Get Probabilities and Confidences ---
print("Getting model predictions and confidences on the test set...")
# Get probabilities for all classes
probas_test = best_svc_model.predict_proba(x_test)
# Get the confidence (probability) of the predicted class
confidences = np.max(probas_test, axis=1)
# Get the predicted class labels
predictions = best_svc_model.predict(x_test)
# Get a boolean array of whether each prediction was correct
accuracies = (predictions == y_test)

# --- Calculate Calibration Curve and ECE ---
# This function bins the confidences and calculates the average accuracy in each bin
fraction_of_positives, mean_predicted_value = calibration_curve(accuracies, confidences, n_bins=10, strategy='uniform')

# Calculate Expected Calibration Error (ECE)
# ECE = sum(|accuracy(bin) - confidence(bin)| * P(bin))
ece_val = 0
bin_boundaries = np.linspace(0, 1, 11)
for i in range(len(mean_predicted_value)):
    bin_start = bin_boundaries[i]
    bin_end = bin_boundaries[i+1]

    # Find indices of samples in this bin
    in_bin = (confidences > bin_start) & (confidences <= bin_end)

    if np.sum(in_bin) > 0:
        # Difference between accuracy and confidence in the bin
        gap = np.abs(np.mean(accuracies[in_bin]) - np.mean(confidences[in_bin]))
        # Weight by the proportion of samples in the bin
        ece_val += gap * (np.sum(in_bin) / len(confidences))

print(f"\nExpected Calibration Error (ECE): {ece_val:.4f}")

# --- Plot the Reliability Diagram ---
print("Generating reliability diagram...")
# Use a default matplotlib style instead of a potentially unavailable seaborn style
plt.style.use('default')
fig, ax = plt.subplots(figsize=(8, 8))

# Plot the calibration curve
ax.plot(mean_predicted_value, fraction_of_positives, "s-", label=f'{MODEL_NAME} (ECE={ece_val:.4f})')
# Plot the perfectly calibrated line
ax.plot([0, 1], [0, 1], "k:", label="Perfectly Calibrated")

ax.set_xlabel("Mean Predicted Confidence", fontsize=12)
ax.set_ylabel("Fraction of Positives (Accuracy)", fontsize=12)
ax.set_title("Reliability Diagram (Calibration Curve)", fontsize=14)
ax.legend(loc='lower right')
ax.grid(True)
plt.tight_layout()

# Save the plot
plt.savefig(PLOT_SAVE_PATH, dpi=300)
print(f"Successfully saved reliability diagram to: {PLOT_SAVE_PATH}")
plt.show()

# --- Save ECE metric ---
calibration_metrics = {'Model': MODEL_NAME, 'ECE': ece_val}
pd.DataFrame([calibration_metrics]).to_csv(METRICS_SAVE_PATH, index=False)
print(f"Calibration metrics saved to: {METRICS_SAVE_PATH}")

from sklearn.calibration import CalibratedClassifierCV

# --- Load your trained SVM model ---
model_path = "/content/drive/MyDrive/PlantWild_Final_Tuned_Experiments/visual_grouping/SVM_best_model.joblib"
svm_model = joblib.load(model_path)

# --- Data Loading Logic (Copying from previous cell) ---
# CHOOSE WHICH EXPERIMENT TO RUN (Ensure this matches the model path)
EXPERIMENT_NAME = "visual_grouping"

# Define paths for loading data
data_paths = {
    "treatment_grouping": '/content/drive/MyDrive/MyPlantDisease_PCA_Data/',
    "visual_grouping": '/content/drive/MyDrive/MyPlantDisease_IncrementalPCA_Data_visual'
}
load_path = data_paths.get(EXPERIMENT_NAME)

if load_path is None or not os.path.exists(load_path):
    raise FileNotFoundError(f"Data path for experiment '{EXPERIMENT_NAME}' not found at '{load_path}'.")

x_train_pool_pca = np.load(os.path.join(load_path, 'x_train_pool_pca.npy'))
y_mlc_train_pool_encoded = np.load(os.path.join(load_path, 'y_mlc_train_pool_encoded.npy'))
x_final_test_pca = np.load(os.path.join(load_path, 'x_final_test_pca.npy'))
y_mlc_final_test_encoded = np.load(os.path.join(load_path, 'y_mlc_final_test_encoded.npy'))

# Create the train/validation/test splits
x_train, x_val, y_train, y_val = train_test_split(
    x_train_pool_pca, y_mlc_train_pool_encoded,
    test_size=0.2, stratify=y_mlc_train_pool_encoded,
    shuffle=True, random_state=42
)
x_test = x_final_test_pca
y_test = y_mlc_final_test_encoded

print("Data loaded and splits recreated successfully for calibration.")
print(f"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}")
print(f"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}")


# --- Apply calibration (choose method: 'sigmoid' or 'isotonic') ---
calibrated_svm = CalibratedClassifierCV(estimator=svm_model, method='isotonic', cv=5)

# --- Fit on your training data again ---
# Use the full training pool (x_train_pool_pca, y_mlc_train_pool_encoded) for calibration fitting
calibrated_svm.fit(x_train_pool_pca, y_mlc_train_pool_encoded)

# --- Evaluate again ---
y_proba_calibrated = calibrated_svm.predict_proba(x_test)
y_pred_calibrated = np.argmax(y_proba_calibrated, axis=1)

# --- Recalculate calibration metrics ---
from sklearn.metrics import accuracy_score, classification_report
acc = accuracy_score(y_test, y_pred_calibrated)
print(f"Accuracy after calibration: {acc:.4f}")

# Optional: Recalculate ECE for the calibrated model
from sklearn.calibration import calibration_curve

# Get the confidence (probability) of the predicted class from the calibrated model
confidences_calibrated = np.max(y_proba_calibrated, axis=1)
# Get a boolean array of whether each prediction was correct
predictions_calibrated = np.argmax(y_proba_calibrated, axis=1)
accuracies_calibrated = (predictions_calibrated == y_test)

# Calculate Calibration Curve and ECE for calibrated model
fraction_of_positives_calibrated, mean_predicted_value_calibrated = calibration_curve(accuracies_calibrated, confidences_calibrated, n_bins=10, strategy='uniform')

ece_calibrated = 0
bin_boundaries = np.linspace(0, 1, 11)
for i in range(len(mean_predicted_value_calibrated)):
    bin_start = bin_boundaries[i]
    bin_end = bin_boundaries[i+1]

    # Find indices of samples in this bin
    in_bin = (confidences_calibrated > bin_start) & (confidences_calibrated <= bin_end)

    if np.sum(in_bin) > 0:
        # Difference between accuracy and confidence in the bin
        gap = np.abs(np.mean(accuracies_calibrated[in_bin]) - np.mean(confidences_calibrated[in_bin]))
        # Weight by the proportion of samples in the bin
        ece_calibrated += gap * (np.sum(in_bin) / len(confidences_calibrated))

print(f"Expected Calibration Error (ECE) after calibration: {ece_calibrated:.4f}")