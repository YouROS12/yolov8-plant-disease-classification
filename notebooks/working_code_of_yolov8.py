# -*- coding: utf-8 -*-
"""Working Code of Yolov8 PlandWild_V2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FumU6bpjC73dPK6d6nhfEx-iRgfImc6q
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install ultralytics

from collections import Counter
import cv2
import glob
import skimage
import numpy as np
import pandas as pd
import seaborn as sn
from tqdm import tqdm
from os import listdir
import matplotlib.pyplot as plt
from skimage.transform import resize
from collections import Counter
import warnings
warnings.filterwarnings("ignore")

import torch
from torchvision import transforms
from PIL import Image, ImageDraw
from pathlib import Path
from sklearn.metrics.pairwise import cosine_similarity
#from ultralytics import YOLO
import pandas as pd
import matplotlib.pyplot as plt
from PIL import Image
from matplotlib.patches import Rectangle
import numpy as np
import seaborn as sns
from matplotlib.colors import to_rgba_array

sn.set()

from sklearn.pipeline import make_pipeline
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectFromModel
from time import time

from sklearn import metrics
from sklearn.utils import shuffle
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score,f1_score,recall_score,cohen_kappa_score,precision_score
from sklearn.utils import compute_class_weight
from sklearn.preprocessing import MinMaxScaler,LabelBinarizer, LabelEncoder
from sklearn.neighbors import KNeighborsClassifier # KNeighborsClassifier
from sklearn.svm import SVC # SVM
from sklearn.ensemble import RandomForestClassifier # RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier # AdaBoostClassifier
from xgboost import XGBClassifier # XGBClassifier
from sklearn.neural_network import MLPClassifier # MLP Classifier
from sklearn.model_selection import train_test_split

# Define hook function
def hook_fn(module, input, output):
    intermediate_features.append(output)

# Define the feature extraction function
def extract_features(model, img, layer_index=8):
    global intermediate_features
    intermediate_features = []
    hook = model.model.model[layer_index].register_forward_hook(hook_fn)
    print(hook)
    with torch.no_grad():
        model(img)
    hook.remove()
    return intermediate_features[0]

#Load the YOLOv8 model
weights_path = '/content/drive/MyDrive/plantwild_split/best.pt'
model = YOLO(weights_path)

# Define the preprocessing function
def preprocess_image(img_path):
    transform = transforms.Compose([
        transforms.Resize((640, 640)),
        #transforms.Grayscale(num_output_channels=3),  # Convert to RGB
        transforms.ToTensor(),
        transforms.Normalize(mean=0., std=1.)
    ])
    img = Image.open(img_path).convert('RGB')
    img = transform(img)
    img = img.unsqueeze(0)

    return img

plant_disease_list = [
    "wheat septoria blotch",
    "zucchini downy mildew",
    "wheat loose smut",
    "zucchini bacterial wilt",
    "zucchini yellow mosaic virus",
    "wheat stripe rust",
    "wheat stem rust",
    "wheat powdery mildew",
    "zucchini powdery mildew",
    "wheat leaf rust",
    "tomato leaf mold",
    "tomato mosaic virus",
    "tomato bacterial leaf spot",
    "tomato yellow leaf curl virus",
    "wheat head scab",
    "wheat bacterial leaf streak (black chaff)",
    "tomato septoria leaf spot",
    "tomato late blight",
    "tobacco mosaic virus",
    "tomato early blight",
    "soybean rust",
    "tobacco frogeye leaf spot",
    "soybean mosaic",
    "soybean frog eye leaf spot",
    "soybean downy mildew",
    "strawberry leaf scorch",
    "tobacco blue mold",
    "tobacco brown spot",
    "squash powdery mildew",
    "strawberry anthracnose",
    "raspberry fire blight",
    "rice sheath blight",
    "soybean bacterial blight",
    "soybean brown spot",
    "raspberry leaf spot",
    "raspberry yellow rust",
    "potato late blight",
    "rice blast",
    "potato early blight",
    "raspberry gray mold",
    "plum pocket disease",
    "peach scab",
    "peach brown rot",
    "peach anthracnose",
    "peach rust",
    "plum pox virus",
    "plum rust",
    "plum brown rot",
    "peach leaf curl",
    "plum bacterial spot",
    "lettuce downy mildew",
    "grape black rot",
    "lettuce mosaic virus",
    "ginger leaf spot",
    "grape downy mildew",
    "maple tar spot",
    "grape leaf spot",
    "grapevine leafroll disease",
    "garlic rust",
    "ginger sheath blight",
    "corn northern leaf blight",
    "cucumber angular leaf spot",
    "corn rust",
    "eggplant phytophthora blight",
    "eggplant phomopsis fruit rot",
    "cucumber powdery mildew",
    "cucumber bacterial wilt",
    "eggplant cercospora leaf spot",
    "corn smut",
    "garlic leaf blight",
    "citrus canker",
    "cherry leaf spot",
    "coffee brown eye spot",
    "coffee black rot",
    "corn gray leaf spot",
    "celery early blight",
    "citrus greening disease",
    "cherry powdery mildew",
    "coffee berry blotch",
    "coffee leaf rust",
    "cauliflower alternaria leaf spot",
    "celery anthracnose",
    "carrot alternaria leaf blight",
    "carrot cercospora leaf blight",
    "cabbage black rot",
    "carrot cavity spot",
    "broccoli ring spot",
    "cabbage downy mildew",
    "cabbage alternaria leaf spot",
    "cauliflower bacterial soft rot",
    "bell pepper blossom end rot",
    "blueberry botrytis blight",
    "bell pepper frogeye leaf spot",
    "broccoli downy mildew",
    "blueberry rust",
    "bell pepper powdery mildew",
    "blueberry anthracnose",
    "blueberry mummy berry",
    "broccoli alternaria leaf spot",
    "blueberry scorch",
    "banana panama disease",
    "bean halo blight",
    "bell pepper bacterial spot",
    "basil downy mildew",
    "bean rust",
    "banana bunchy top",
    "banana black leaf streak",
    "banana cordana leaf spot",
    "bean mosaic virus",
    "banana cigar end rot",
    "apple mosaic virus",
    "banana anthracnose",
    "apple scab",
    "apple black rot",
    "apple rust"
]

"""**Working Part**"""

train_data = []
test_data = []

for disease in plant_disease_list:
    train_path = f'/content/drive/MyDrive/plantwild_split/train/{disease}/*.jpg'
    test_path = f'/content/drive/MyDrive/plantwild_split/test/{disease}/*.jpg'

    for img_path in glob.glob(train_path):
        img = preprocess_image(img_path)
        features = extract_features(model, img, layer_index=8)
        train_data.append({'image': features.cpu().numpy(), 'label': disease})
        print(disease)

    for img_path in glob.glob(test_path):
        img = preprocess_image(img_path)
        features = extract_features(model, img, layer_index=8)
        test_data.append({'image': features.cpu().numpy(), 'label': disease})
        print(disease)

"""**New data flow**

Extract Features in batch
"""

import pandas as pd
import numpy as np
import glob
from tqdm import tqdm
from PIL import Image
from torchvision import transforms
import torch

# --- Part 1: Helper Functions for Fast Batch Processing ---

def preprocess_single_image_for_batching(img_path):
    """
    Preprocesses a single image from a path for use in a batch.
    It does NOT add the batch dimension.
    """
    transform = transforms.Compose([
        transforms.Resize((640, 640)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0., 0., 0.], std=[1., 1., 1.])
    ])
    try:
        img = Image.open(img_path).convert('RGB')
        return transform(img)
    except Exception as e:
        print(f"Warning: Could not process image {img_path}. Skipping. Error: {e}")
        return None

def process_in_batches_and_get_labels(file_paths, corresponding_labels, batch_size, model_to_use, layer_idx):
    """
    Processes files in batches, extracts features, and returns a list of feature arrays
    AND a corresponding list of labels for the successfully processed images.
    """
    successful_features = []
    successful_labels = []

    for i in tqdm(range(0, len(file_paths), batch_size), desc="Processing Batches"):
        batch_paths = file_paths[i:i+batch_size]
        batch_initial_labels = corresponding_labels[i:i+batch_size]

        processed_images = [preprocess_single_image_for_batching(p) for p in batch_paths]

        # Create lists of valid tensors and their corresponding labels
        valid_tensors = []
        valid_labels_for_batch = []
        for img_tensor, label in zip(processed_images, batch_initial_labels):
            if img_tensor is not None:
                valid_tensors.append(img_tensor)
                valid_labels_for_batch.append(label)

        if not valid_tensors:
            continue

        batch_tensor = torch.stack(valid_tensors).to(model_to_use.device)

        # This assumes your 'extract_features' function is defined elsewhere
        batch_features = extract_features(model_to_use, batch_tensor, layer_index=layer_idx)

        # Add the extracted features and their corresponding labels to our master lists
        successful_features.extend(list(batch_features.cpu().numpy()))
        successful_labels.extend(valid_labels_for_batch)

    return successful_features, successful_labels


# --- Part 2: Main Logic to Re-create `train_data` and `test_data` ---

# Ensure these are defined from your setup cells
# model = YOLO(...)
# plant_disease_list = [...]
# def extract_features(...): ...

# -- TUNE THESE AS NEEDED --
BATCH_SIZE = 32
LAYER_INDEX_TO_EXTRACT = 8 # Or whichever layer index you've settled on

# 1. Collect all file paths and their corresponding labels first
print("Collecting all file paths...")
all_train_paths = []
all_train_labels = []
all_test_paths = []
all_test_labels = []

for disease in tqdm(plant_disease_list, desc="Scanning diseases"):
    train_paths_for_disease = glob.glob(f'/content/drive/MyDrive/plantwild_split/train/{disease}/*.jpg')
    all_train_paths.extend(train_paths_for_disease)
    all_train_labels.extend([disease] * len(train_paths_for_disease))

    test_paths_for_disease = glob.glob(f'/content/drive/MyDrive/plantwild_split/test/{disease}/*.jpg')
    all_test_paths.extend(test_paths_for_disease)
    all_test_labels.extend([disease] * len(test_paths_for_disease))

print(f"\nFound {len(all_train_paths)} training images and {len(all_test_paths)} testing images.")

# 2. Process all images and get feature/label pairs
print("\nExtracting features from training images...")
train_features, train_labels = process_in_batches_and_get_labels(
    all_train_paths, all_train_labels, BATCH_SIZE, model, LAYER_INDEX_TO_EXTRACT
)

print("\nExtracting features from testing images...")
test_features, test_labels = process_in_batches_and_get_labels(
    all_test_paths, all_test_labels, BATCH_SIZE, model, LAYER_INDEX_TO_EXTRACT
)

# 3. Re-create your original `train_data` and `test_data` lists
#    This loop creates the list of dictionaries in the exact format you need.
train_data = [{'image': feature, 'label': label} for feature, label in zip(train_features, train_labels)]
test_data = [{'image': feature, 'label': label} for feature, label in zip(test_features, test_labels)]


# --- Part 3: Verification ---
print("\n--- Feature Extraction Complete ---")
print(f"Number of items in 'train_data' list: {len(train_data)}")
print(f"Number of items in 'test_data' list: {len(test_data)}")

# Verify the format of the first item
if train_data:
    first_item = train_data[0]
    print(f"Format of first item in train_data: Keys={list(first_item.keys())}")
    print(f"Shape of 'image' feature in first item: {first_item['image'].shape}")
    print(f"Label of first item: {first_item['label']}")

df_train_original = pd.DataFrame(train_data)
df_test_original = pd.DataFrame(test_data)

print("Original df_train_original shape:", df_train_original.shape)
print("Original df_test_original shape:", df_test_original.shape)
if not df_train_original.empty:
    print("Labels Count in df_train_original:", Counter(df_train_original['label']))
if not df_test_original.empty:
    print("Labels Count in df_test_original:", Counter(df_test_original['label']))

import pandas as pd
import os

drive_save_path = '/content/drive/MyDrive/plantwild_split' # CHANGE AS NEEDED
os.makedirs(drive_save_path, exist_ok=True) # Create the directory if it doesn't exist

# Define the full file paths
train_pickle_path = os.path.join(drive_save_path, 'df_train_features.pkl')
test_pickle_path = os.path.join(drive_save_path, 'df_test_features.pkl')

# --- Save the entire DataFrame objects using pickle ---
# This preserves the NumPy arrays in the 'image' column perfectly, with no data loss.
print(f"Saving training DataFrame to: {train_pickle_path}")
df_train_original.to_pickle(train_pickle_path)

print(f"Saving testing DataFrame to: {test_pickle_path}")
df_test_original.to_pickle(test_pickle_path)

print("\nDataFrames saved successfully as pickle files.")
print("These files contain the full, untruncated feature arrays and can be reloaded perfectly.")

"""Load Pre-Extracted Features from Pickle Files


"""

import pandas as pd
import numpy as np
import pickle
import os

# --- Configuration ---
# Set this to True if you need to load from .pkl, False if running feature extraction fresh.
LOAD_FROM_PKL = True
drive_path = '/content/drive/MyDrive/plantwild_split' # Your main Google Drive path
train_pkl_path = os.path.join(drive_path, 'df_train_features.pkl') # Path to your saved train .pkl file
test_pkl_path = os.path.join(drive_path, 'df_test_features.pkl')   # Path to your saved test .pkl file

if LOAD_FROM_PKL:
    print("--- Loading pre-extracted features from Pickle (.pkl) files ---")
    try:
        # Load the lists of dictionaries from the .pkl files
        with open(train_pkl_path, 'rb') as f:
            train_data = pickle.load(f) # This is your list of dicts

        with open(test_pkl_path, 'rb') as f:
            test_data = pickle.load(f) # This is your list of dicts

        # Now, create the DataFrames directly from the loaded lists
        df_train_original = pd.DataFrame(train_data)
        df_test_original = pd.DataFrame(test_data)

        print(f"Successfully loaded {len(df_train_original)} training samples and {len(df_test_original)} test samples.")
        if not df_train_original.empty:
            print("Sample of loaded training data:")
            print(df_train_original.head())
            print("Data type of 'image' column in train_df:", type(df_train_original['image'].iloc[0]))
        else:
            print("Warning: Loaded training DataFrame is empty.")

    except FileNotFoundError:
        print("ERROR: Pickle files not found. Please ensure these paths are correct:")
        print(f"Train PKL: {train_pkl_path}")
        print(f"Test PKL: {test_pkl_path}")
        print("Set LOAD_FROM_PKL to False to run feature extraction again.")
        raise # Stop execution
    except Exception as e:
        print(f"An error occurred while loading the pickle files: {e}")
        raise

"""Define and Apply Class Mapping

Prepare Data, Scale, and Apply PCA
"""

from sklearn.decomposition import IncrementalPCA
import numpy as np

print("--- Preparing Data and Applying Incremental PCA ---")

x_train_pool_reshaped = np.stack(df_train_original['image'].tolist()).reshape(len(df_train_original), -1)
x_final_test_reshaped = np.stack(df_test_original['image'].tolist()).reshape(len(df_test_original), -1)

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y_mlc_train_pool_encoded = label_encoder.fit_transform(df_train_original['grouped_label'])
y_mlc_final_test_encoded = label_encoder.transform(df_test_original['grouped_label'])

n_samples = x_train_pool_reshaped.shape[0]
batch_size = max(500, n_samples // 20) # Use a reasonable batch size
n_components = min(450, n_samples) # Must be smaller than batch_size

print(f"Original feature shape: {x_train_pool_reshaped.shape}")
print(f"Using IncrementalPCA with n_components={n_components} and batch_size={batch_size}")

inc_pca = IncrementalPCA(n_components=n_components, batch_size=batch_size)
inc_pca.fit(x_train_pool_reshaped)

print("\nTransforming data using the fitted IncrementalPCA model...")
x_train_pool_pca = inc_pca.transform(x_train_pool_reshaped)
x_final_test_pca = inc_pca.transform(x_final_test_reshaped)

print(f"New feature shape after IncrementalPCA: {x_train_pool_pca.shape}")

"""Save PCA Results and Transformation Objects"""

import pandas as pd
import numpy as np

print("="*80)
print("= STARTING EXPERIMENT: VISUAL-BASED GROUPING (19 Super-Classes) =")
print("="*80)

visual_class_mapping = {
    "wheat stripe rust": "rust", "wheat stem rust": "rust", "wheat leaf rust": "rust", "soybean rust": "rust", "raspberry yellow rust": "rust", "peach rust": "rust", "plum rust": "rust", "garlic rust": "rust", "corn rust": "rust", "coffee leaf rust": "rust", "blueberry rust": "rust", "bean rust": "rust", "apple rust": "rust",
    "wheat powdery mildew": "powdery_mildew", "zucchini powdery mildew": "powdery_mildew", "squash powdery mildew": "powdery_mildew", "cherry powdery mildew": "powdery_mildew", "cucumber powdery mildew": "powdery_mildew", "bell pepper powdery mildew": "powdery_mildew",
    "zucchini downy mildew": "downy_mildew", "soybean downy mildew": "downy_mildew", "lettuce downy mildew": "downy_mildew", "grape downy mildew": "downy_mildew", "cabbage downy mildew": "downy_mildew", "broccoli downy mildew": "downy_mildew", "basil downy mildew": "downy_mildew",
    "tomato late blight": "blight", "tomato early blight": "blight", "raspberry fire blight": "blight", "potato late blight": "blight", "potato early blight": "blight", "corn northern leaf blight": "blight", "eggplant phytophthora blight": "blight", "garlic leaf blight": "blight", "celery early blight": "blight", "carrot alternaria leaf blight": "blight", "blueberry botrytis blight": "blight", "bean halo blight": "blight", "soybean bacterial blight": "blight", "ginger sheath blight": "blight", "rice sheath blight": "blight", "rice blast": "blight",
    "tomato bacterial leaf spot": "leaf_spot", "tomato septoria leaf spot": "leaf_spot", "tobacco frogeye leaf spot": "leaf_spot", "soybean frog eye leaf spot": "leaf_spot", "tobacco brown spot": "leaf_spot", "soybean brown spot": "leaf_spot", "raspberry leaf spot": "leaf_spot", "plum bacterial spot": "leaf_spot", "ginger leaf spot": "leaf_spot", "grape leaf spot": "leaf_spot", "cucumber angular leaf spot": "leaf_spot", "eggplant cercospora leaf spot": "leaf_spot", "cherry leaf spot": "leaf_spot", "coffee brown eye spot": "leaf_spot", "corn gray leaf spot": "leaf_spot", "cauliflower alternaria leaf spot": "leaf_spot", "carrot cercospora leaf blight": "leaf_spot", "broccoli ring spot": "leaf_spot", "cabbage alternaria leaf spot": "leaf_spot", "bell pepper frogeye leaf spot": "leaf_spot", "broccoli alternaria leaf spot": "leaf_spot", "bell pepper bacterial spot": "leaf_spot", "banana black leaf streak": "leaf_spot", "banana cordana leaf spot": "leaf_spot", "maple tar spot": "leaf_spot",
    "zucchini yellow mosaic virus": "virus", "tomato mosaic virus": "virus", "tomato yellow leaf curl virus": "virus", "tobacco mosaic virus": "virus", "soybean mosaic": "virus", "plum pox virus": "virus", "lettuce mosaic virus": "virus", "grapevine leafroll disease": "virus", "citrus greening disease": "virus", "banana bunchy top": "virus", "bean mosaic virus": "virus", "apple mosaic virus": "virus",
    "peach brown rot": "rot", "plum brown rot": "rot", "grape black rot": "rot", "eggplant phomopsis fruit rot": "rot", "coffee black rot": "rot", "cabbage black rot": "rot", "cauliflower bacterial soft rot": "rot", "bell pepper blossom end rot": "rot", "banana cigar end rot": "rot", "apple black rot": "rot",
    "strawberry anthracnose": "anthracnose", "peach anthracnose": "anthracnose", "celery anthracnose": "anthracnose", "blueberry anthracnose": "anthracnose", "banana anthracnose": "anthracnose",
    "wheat head scab": "scab", "peach scab": "scab", "apple scab": "scab",
    "zucchini bacterial wilt": "bacterial_disease", "wheat bacterial leaf streak (black chaff)": "bacterial_disease", "cucumber bacterial wilt": "bacterial_disease", "citrus canker": "bacterial_disease",
    "tomato leaf mold": "mold", "tobacco blue mold": "mold", "raspberry gray mold": "mold",
    "wheat septoria blotch": "septoria_blotch",
    "wheat loose smut": "smut", "corn smut": "smut",
    "strawberry leaf scorch": "leaf_scorch", "blueberry scorch": "leaf_scorch",
    "plum pocket disease": "fungal_gall", "peach leaf curl": "fungal_gall",
    "blueberry mummy berry": "mummy_berry",
    "banana panama disease": "fungal_wilt",
    "carrot cavity spot": "oomycete_lesion",
    "coffee berry blotch": "berry_blotch",
}

print("--- Applying Visual Class Mapping ---")
df_train_original['grouped_label'] = df_train_original['label'].map(visual_class_mapping).fillna('unmapped')
df_test_original['grouped_label'] = df_test_original['label'].map(visual_class_mapping).fillna('unmapped')

if (df_train_original['grouped_label'] == 'unmapped').any() or (df_test_original['grouped_label'] == 'unmapped').any():
    print("WARNING: Some labels were not found in the mapping dictionary!")
else:
    print("Visual class mapping applied successfully.")
    print(f"New number of classes for this experiment: {df_train_original['grouped_label'].nunique()}")

import joblib
import numpy as np
import os

drive_save_path_pca = '/content/drive/MyDrive/MyPlantDisease_IncrementalPCA_Data_visual/'
os.makedirs(drive_save_path_pca, exist_ok=True)
print(f"\n--- Saving Incremental PCA data and models to: {drive_save_path_pca} ---")

try:
    np.save(os.path.join(drive_save_path_pca, 'x_train_pool_pca.npy'), x_train_pool_pca)
    np.save(os.path.join(drive_save_path_pca, 'x_final_test_pca.npy'), x_final_test_pca)
    np.save(os.path.join(drive_save_path_pca, 'y_mlc_train_pool_encoded.npy'), y_mlc_train_pool_encoded)
    np.save(os.path.join(drive_save_path_pca, 'y_mlc_final_test_encoded.npy'), y_mlc_final_test_encoded)

    joblib.dump(inc_pca, os.path.join(drive_save_path_pca, 'inc_pca.joblib'))
    joblib.dump(label_encoder, os.path.join(drive_save_path_pca, 'label_encoder.joblib'))

    print("All Incremental PCA data and transformation models saved successfully.")
except Exception as e:
    print(f"An error occurred during saving: {e}")

"""Create Final Train, Validation, and Test Sets"""

from sklearn.model_selection import train_test_split

print("--- Creating Final Data Splits for Model Training ---")

x_train_acc, x_val_acc, y_train_acc, y_val_acc = train_test_split(
    x_train_pool_pca,
    y_mlc_train_pool_encoded,
    test_size=0.2,
    stratify=y_mlc_train_pool_encoded,
    shuffle=True,
    random_state=42
)

X_test = x_final_test_pca
y_test = y_mlc_final_test_encoded

print(f'x_train_acc (for MLC training): {x_train_acc.shape}')
print(f'x_val_acc (for MLC validation): {x_val_acc.shape}')
print(f'X_test (for MLC final test): {X_test.shape}')

"""Cell to Load Data in a New Session (For Future Use)"""

import joblib
import numpy as np
import os
from sklearn.model_selection import train_test_split

LOAD_FROM_INC_PCA_FILES = True
drive_load_path_pca = '/content/drive/MyDrive/MyPlantDisease_IncrementalPCA_Data'

if LOAD_FROM_INC_PCA_FILES:
    print(f"--- Loading pre-computed Incremental PCA data from: {drive_load_path_pca} ---")
    try:
        x_train_pool_pca = np.load(os.path.join(drive_load_path_pca, 'x_train_pool_pca.npy'))
        x_final_test_pca = np.load(os.path.join(drive_load_path_pca, 'x_final_test_pca.npy'))
        y_mlc_train_pool_encoded = np.load(os.path.join(drive_load_path_pca, 'y_mlc_train_pool_encoded.npy'))
        y_mlc_final_test_encoded = np.load(os.path.join(drive_load_path_pca, 'y_mlc_final_test_encoded.npy'))

        inc_pca = joblib.load(os.path.join(drive_load_path_pca, 'inc_pca.joblib'))
        label_encoder = joblib.load(os.path.join(drive_load_path_pca, 'label_encoder.joblib'))
        print("Loaded data arrays and transformation models successfully.")

        x_train_acc, x_val_acc, y_train_acc, y_val_acc = train_test_split(
            x_train_pool_pca, y_mlc_train_pool_encoded,
            test_size=0.2, stratify=y_mlc_train_pool_encoded,
            shuffle=True, random_state=42
        )

        X_test = x_final_test_pca
        y_test = y_mlc_final_test_encoded

        print('\n--- Data successfully loaded and prepared from saved Incremental PCA files. ---')
        print(f'x_train_acc shape: {x_train_acc.shape}')
        print(f'X_test shape: {X_test.shape}')
    except Exception as e:
        print(f"An error occurred during loading: {e}")
        raise

"""Define the Summary Function for Training and Evaluation"""

import numpy as np
from sklearn.metrics import (confusion_matrix, classification_report, accuracy_score,
                             f1_score, recall_score, cohen_kappa_score, precision_score)
from sklearn.pipeline import Pipeline
import pandas as pd
import joblib
import os

def print_confusion_matrix_summary(set_name, cm):
    print(f"\n--- {set_name} Set Confusion Matrix ---")
    if cm.shape[0] <= 15:
        print(cm)
    else:
        print(f"(Confusion matrix is {cm.shape[0]}x{cm.shape[0]}, too large to print fully here)")

def classifier_summary(pipeline, model_name_func, X_train_func, y_train_func, X_val_func, y_val_func, X_test_func, y_test_func):
    print(f"\n--- Processing: {model_name_func} ---")
    if not (hasattr(X_train_func, 'shape') and hasattr(y_train_func, 'shape') and
            X_train_func.shape[0] > 0 and y_train_func.shape[0] > 0 and
            X_train_func.shape[0] == y_train_func.shape[0]):
        print(f"ERROR: Inconsistent lengths or empty array for TRAINING data for {model_name_func}. Skipping.")
        return None, None

    print(f"Fitting {model_name_func}...")
    sentiment_fit = pipeline.fit(X_train_func, y_train_func)

    y_pred_train = sentiment_fit.predict(X_train_func)
    y_pred_val = sentiment_fit.predict(X_val_func) if X_val_func.size > 0 else np.array([])
    y_pred_test = sentiment_fit.predict(X_test_func) if X_test_func.size > 0 else np.array([])

    metrics_results = {'model_name': model_name_func}

    print('\n------------------------ Train Set Metrics------------------------')
    if y_train_func.size > 0 and y_pred_train.size > 0:
        metrics_results['train_accuracy'] = np.round(accuracy_score(y_train_func, y_pred_train),4)*100
        metrics_results['train_precision'] = np.round(precision_score(y_train_func, y_pred_train, average='weighted', zero_division=0),4)
        metrics_results['train_recall'] = np.round(recall_score(y_train_func, y_pred_train, average='weighted', zero_division=0),4)
        metrics_results['train_F1'] = np.round(f1_score(y_train_func, y_pred_train, average='weighted', zero_division=0),4)
        metrics_results['train_kappa'] =  np.round(cohen_kappa_score(y_train_func, y_pred_train),4)
        print(f"accuracy : {metrics_results['train_accuracy']}%")
        print(f"F1_score : {metrics_results['train_F1']}")
        print(f"Cohen Kappa Score : {metrics_results['train_kappa']}")
        print(f"Recall : {metrics_results['train_recall']}")
        print(f"Precision : {metrics_results['train_precision']}")
        print_confusion_matrix_summary("Train", confusion_matrix(y_train_func,y_pred_train))

    print('\n------------------------ Validation Set Metrics------------------------')
    if y_val_func.size > 0 and y_pred_val.size > 0:
        metrics_results['val_accuracy'] = np.round(accuracy_score(y_val_func, y_pred_val),4)*100
        metrics_results['val_precision'] = np.round(precision_score(y_val_func, y_pred_val, average='weighted', zero_division=0),4)
        metrics_results['val_recall'] = np.round(recall_score(y_val_func, y_pred_val, average='weighted', zero_division=0),4)
        metrics_results['val_F1'] = np.round(f1_score(y_val_func, y_pred_val, average='weighted', zero_division=0),4)
        metrics_results['val_kappa'] =  np.round(cohen_kappa_score(y_val_func, y_pred_val),4)
        print(f"accuracy : {metrics_results['val_accuracy']}%")
        print(f"F1_score : {metrics_results['val_F1']}")
        print(f"Cohen Kappa Score : {metrics_results['val_kappa']}")
        print(f"Recall : {metrics_results['val_recall']}")
        print(f"Precision : {metrics_results['val_precision']}")
        print_confusion_matrix_summary("Validation", confusion_matrix(y_val_func,y_pred_val))

    print('\n------------------------ Test Set Metrics------------------------')
    if y_test_func.size > 0 and y_pred_test.size > 0:
        metrics_results['test_accuracy'] = np.round(accuracy_score(y_test_func, y_pred_test),4)*100
        metrics_results['test_precision'] = np.round(precision_score(y_test_func, y_pred_test, average='weighted', zero_division=0),4)
        metrics_results['test_recall'] = np.round(recall_score(y_test_func, y_pred_test, average='weighted', zero_division=0),4)
        metrics_results['test_F1'] = np.round(f1_score(y_test_func, y_pred_test, average='weighted', zero_division=0),4)
        metrics_results['test_kappa'] =  np.round(cohen_kappa_score(y_test_func, y_pred_test),4)
        print(f"accuracy : {metrics_results['test_accuracy']}%")
        print(f"F1_score : {metrics_results['test_F1']}")
        print(f"Cohen Kappa Score : {metrics_results['test_kappa']}")
        print(f"Recall : {metrics_results['test_recall']}")
        print(f"Precision : {metrics_results['test_precision']}")
        print_confusion_matrix_summary("Test", confusion_matrix(y_test_func,y_pred_test))

    print("-"*80)
    print()
    return metrics_results, sentiment_fit

"""Setup Save Paths and Verify Data"""

drive_save_path = '/content/drive/MyDrive/MyPlantDiseaseModels_PCA_Grouped/'
os.makedirs(drive_save_path, exist_ok=True)

if not ( 'x_train_acc' in locals() and 'y_train_acc' in locals() and
         'x_val_acc' in locals() and 'y_val_acc' in locals() and
         'X_test' in locals() and 'y_test' in locals() and
         x_train_acc.shape[0] == y_train_acc.shape[0] and
         x_val_acc.shape[0] == y_val_acc.shape[0] and
         X_test.shape[0] == y_test.shape[0]):
    print("CRITICAL WARNING: Data split variables (x_train_acc, etc.) are not defined or have inconsistent shapes.")
    raise ValueError("Input data for model training is not correctly prepared. Rerun data preparation cells (1-5).")
else:
    print("Data variables are defined and consistent. Ready to run individual model training cells.")

"""K Nearest Neighbour Classifier"""

from sklearn.neighbors import KNeighborsClassifier

model_name_file = "K_Nearest_Neighbour_Classifier"
model_name_display = "K Nearest Neighbour Classifier"

print(f"\n--- Starting: {model_name_display} ---")

try:
    classifier_instance = KNeighborsClassifier(n_jobs=-1)
    pipeline_instance = Pipeline([('classifier', classifier_instance)])

    metrics_dict, fitted_pipeline = classifier_summary(
        pipeline_instance, model_name_display,
        x_train_acc, y_train_acc, x_val_acc, y_val_acc, X_test, y_test
    )

    if metrics_dict and fitted_pipeline:
        pd.DataFrame([metrics_dict]).to_csv(os.path.join(drive_save_path, f'{model_name_file}_metrics.csv'), index=False)
        joblib.dump(fitted_pipeline, os.path.join(drive_save_path, f'{model_name_file}_model.joblib'))
        print(f"Successfully saved metrics and model for {model_name_display}.")
except Exception as e:
    print(f"An error occurred for {model_name_display}: {e}")

"""SVC Classifier"""

from sklearn.svm import SVC

model_name_file = "SVC_Classifier"
model_name_display = "SVC Classifier"

print(f"\n--- Starting: {model_name_display} ---")

try:
    classifier_instance = SVC(
        C=1,
        gamma='scale',
        probability=True,
        random_state=42
    )
    pipeline_instance = Pipeline([('classifier', classifier_instance)])

    metrics_dict, fitted_pipeline = classifier_summary(
        pipeline_instance, model_name_display,
        x_train_acc, y_train_acc, x_val_acc, y_val_acc, X_test, y_test
    )

    if metrics_dict and fitted_pipeline:
        pd.DataFrame([metrics_dict]).to_csv(
            os.path.join(drive_save_path, f'{model_name_file}_metrics.csv'), index=False
        )
        joblib.dump(
            fitted_pipeline, os.path.join(drive_save_path, f'{model_name_file}_model.joblib')
        )
        print(f"Successfully saved metrics and model for {model_name_display}.")
except Exception as e:
    print(f"An error occurred for {model_name_display}: {e}")

"""Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier

model_name_file = "Random_Forest_Classifier"
model_name_display = "Random Forest Classifier"

print(f"\n--- Starting: {model_name_display} ---")

try:
    classifier_instance = RandomForestClassifier(random_state=42, n_jobs=-1)
    pipeline_instance = Pipeline([('classifier', classifier_instance)])

    metrics_dict, fitted_pipeline = classifier_summary(
        pipeline_instance, model_name_display,
        x_train_acc, y_train_acc, x_val_acc, y_val_acc, X_test, y_test
    )

    if metrics_dict and fitted_pipeline:
        pd.DataFrame([metrics_dict]).to_csv(os.path.join(drive_save_path, f'{model_name_file}_metrics.csv'), index=False)
        joblib.dump(fitted_pipeline, os.path.join(drive_save_path, f'{model_name_file}_model.joblib'))
        print(f"Successfully saved metrics and model for {model_name_display}.")
except Exception as e:
    print(f"An error occurred for {model_name_display}: {e}")

"""Improved Random Forest"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
import pandas as pd
import joblib
import os

model_name_file = "Random_Forest_Tuned"
model_name_display = "Random Forest (Tuned)"

print(f"\n--- Starting Hyperparameter Tuning for: {model_name_display} ---")

try:
    if not ('x_train_acc' in locals() and x_train_acc.size > 0):
        raise ValueError("x_train_acc is not defined or is empty.")

    # Define the pipeline with the classifier
    pipeline = Pipeline([
        ('classifier', RandomForestClassifier(random_state=42, n_jobs=-1))
    ])

    # Define the grid of parameters to search.
    # We focus on parameters that control tree complexity to prevent overfitting.
    param_grid = {
        'classifier__n_estimators': [100, 200],  # Number of trees
        'classifier__max_depth': [10, 20, 30], # Maximum depth of the trees
        'classifier__min_samples_leaf': [5, 10]  # Minimum samples required at a leaf node
    }

    # Create the GridSearchCV object.
    # cv=3 means 3-fold cross-validation.
    # scoring='accuracy' tells it to find the parameters that maximize accuracy.
    # verbose=2 will print progress updates.
    grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy', verbose=2, n_jobs=-1)

    print("Fitting GridSearchCV... This may take a while.")
    # Fit the search on the training data. It will handle the cross-validation automatically.
    grid_search.fit(x_train_acc, y_train_acc)

    print("\nBest parameters found: ", grid_search.best_params_)
    print("Best cross-validation accuracy: {:.2f}%".format(grid_search.best_score_ * 100))

    # Use the best model found by the grid search to get full metrics
    best_model_pipeline = grid_search.best_estimator_

    print("\n--- Evaluating Best Tuned Random Forest Model ---")
    metrics_dict, _ = classifier_summary(
        best_model_pipeline, model_name_display,
        x_train_acc, y_train_acc, x_val_acc, y_val_acc, X_test, y_test
    )

    # Save the results and the best model
    if metrics_dict and best_model_pipeline:
        pd.DataFrame([metrics_dict]).to_csv(os.path.join(drive_save_path, f'{model_name_file}_metrics.csv'), index=False)
        joblib.dump(best_model_pipeline, os.path.join(drive_save_path, f'{model_name_file}_model.joblib'))
        print(f"Successfully saved tuned metrics and model for {model_name_display}.")

except Exception as e:
    print(f"An error occurred during Random Forest tuning: {e}")

"""Improved Random Forest 2"""

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform


print("Setup complete for RandomizedSearchCV.")

# Cell 10.1: Tune Random Forest with RandomizedSearchCV

from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline

model_name_file = "Random_Forest_RandomTuned"
model_name_display = "Random Forest (Randomized Tuned)"

print(f"\n--- Starting Hyperparameter Tuning for: {model_name_display} ---")

try:
    pipeline = Pipeline([
        ('classifier', RandomForestClassifier(random_state=42, n_jobs=-1))
    ])

    # Define a "distribution" of parameters to sample from.
    # This is a much wider range than the previous GridSearchCV.
    param_dist = {
        'classifier__n_estimators': randint(100, 600),             # Number of trees in the forest
        'classifier__max_depth': [10, 20, 30, 40, None],          # Max depth of the tree (None means unlimited)
        'classifier__min_samples_split': randint(2, 20),          # Min samples to split a node
        'classifier__min_samples_leaf': randint(2, 20),           # Min samples at a leaf node
        'classifier__max_features': ['sqrt', 'log2', None],       # Number of features to consider for best split
        'classifier__class_weight': ['balanced', 'balanced_subsample', None] # To handle imbalanced data
    }

    # Create the RandomizedSearchCV object
    # n_iter=50 means it will try 50 different random combinations of parameters.
    # cv=3 means 3-fold cross-validation.
    # verbose=2 will print progress.
    random_search_rf = RandomizedSearchCV(
        pipeline,
        param_distributions=param_dist,
        n_iter=50,  # Increase or decrease based on how much time you have
        cv=3,
        scoring='accuracy',
        verbose=2,
        random_state=42,
        n_jobs=-1
    )

    print("Fitting RandomizedSearchCV for Random Forest... This may take a while.")
    random_search_rf.fit(x_train_acc, y_train_acc)

    print("\nBest parameters found: ", random_search_rf.best_params_)
    print("Best cross-validation accuracy: {:.2f}%".format(random_search_rf.best_score_ * 100))

    best_model_rf = random_search_rf.best_estimator_

    print("\n--- Evaluating Best Tuned Random Forest Model ---")
    metrics_dict_rf, _ = classifier_summary(
        best_model_rf, model_name_display,
        x_train_acc, y_train_acc, x_val_acc, y_val_acc, X_test, y_test
    )

    if metrics_dict_rf:
        pd.DataFrame([metrics_dict_rf]).to_csv(os.path.join(drive_save_path, f'{model_name_file}_metrics.csv'), index=False)
        joblib.dump(best_model_rf, os.path.join(drive_save_path, f'{model_name_file}_model.joblib'))
        print(f"Successfully saved tuned metrics and model for {model_name_display}.")

except Exception as e:
    print(f"An error occurred during Random Forest tuning: {e}")

"""AdaBoost Classifier"""

from sklearn.ensemble import AdaBoostClassifier

model_name_file = "AdaBoost_Classifier"
model_name_display = "AdaBoost Classifier"

print(f"\n--- Starting: {model_name_display} ---")

try:
    classifier_instance = AdaBoostClassifier(random_state=42)
    pipeline_instance = Pipeline([('classifier', classifier_instance)])

    metrics_dict, fitted_pipeline = classifier_summary(
        pipeline_instance, model_name_display,
        x_train_acc, y_train_acc, x_val_acc, y_val_acc, X_test, y_test
    )

    if metrics_dict and fitted_pipeline:
        pd.DataFrame([metrics_dict]).to_csv(os.path.join(drive_save_path, f'{model_name_file}_metrics.csv'), index=False)
        joblib.dump(fitted_pipeline, os.path.join(drive_save_path, f'{model_name_file}_model.joblib'))
        print(f"Successfully saved metrics and model for {model_name_display}.")
except Exception as e:
    print(f"An error occurred for {model_name_display}: {e}")

"""Improved AdaBoost"""

from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
import pandas as pd
import joblib
import os

model_name_file = "AdaBoost_Tuned"
model_name_display = "AdaBoost (Tuned)"

print(f"\n--- Starting Hyperparameter Tuning for: {model_name_display} ---")

try:
    if not ('x_train_acc' in locals() and x_train_acc.size > 0):
        raise ValueError("x_train_acc is not defined or is empty.")

    # NOTE: AdaBoost's 'classifier' is the final model, but we define its base estimator inside.
    # We will tune the base estimator's complexity.
    pipeline = Pipeline([
        ('classifier', AdaBoostClassifier(random_state=42))
    ])

    # The key to improving AdaBoost is giving it a better `base_estimator`.
    # The default is a DecisionTreeClassifier(max_depth=1), which is too simple.
    param_grid = {
        'classifier__base_estimator': [
            DecisionTreeClassifier(max_depth=1),  # The original default
            DecisionTreeClassifier(max_depth=2),  # A slightly more complex tree
            DecisionTreeClassifier(max_depth=3)   # Even more complex
        ],
        'classifier__n_estimators': [50, 100] # Number of boosting stages
    }

    # In newer scikit-learn versions, 'base_estimator' is deprecated in favor of 'estimator'.
    # If you get a warning, change 'classifier__base_estimator' to 'classifier__estimator'.
    # param_grid = {
    #     'classifier__estimator': [ ... ],
    #     'classifier__n_estimators': [50, 100]
    # }

    grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy', verbose=2, n_jobs=-1)

    print("Fitting GridSearchCV... This may take a while.")
    grid_search.fit(x_train_acc, y_train_acc)

    print("\nBest parameters found: ", grid_search.best_params_)
    print("Best cross-validation accuracy: {:.2f}%".format(grid_search.best_score_ * 100))

    best_model_pipeline = grid_search.best_estimator_

    print("\n--- Evaluating Best Tuned AdaBoost Model ---")
    metrics_dict, _ = classifier_summary(
        best_model_pipeline, model_name_display,
        x_train_acc, y_train_acc, x_val_acc, y_val_acc, X_test, y_test
    )

    if metrics_dict and best_model_pipeline:
        pd.DataFrame([metrics_dict]).to_csv(os.path.join(drive_save_path, f'{model_name_file}_metrics.csv'), index=False)
        joblib.dump(best_model_pipeline, os.path.join(drive_save_path, f'{model_name_file}_model.joblib'))
        print(f"Successfully saved tuned metrics and model for {model_name_display}.")

except Exception as e:
    print(f"An error occurred during AdaBoost tuning: {e}")

"""XGBoost"""

from xgboost import XGBClassifier
from sklearn.pipeline import Pipeline
import pandas as pd
import joblib
import os

model_name_file = "XGB_Classifier"
model_name_display = "XGB Classifier"

print(f"\n--- Starting: {model_name_display} ---")

try:
    if not ('x_train_acc' in locals() and x_train_acc.size > 0):
        raise ValueError("x_train_acc is not defined or is empty.")

    # Instantiate the XGBClassifier.
    # XGBoost can use the GPU if available and properly configured, which can be much faster.
    # use_label_encoder=False is recommended to avoid deprecation warnings.
    # eval_metric='mlogloss' is standard for multi-class classification.
    # n_jobs=-1 will use all available CPU cores.
    classifier_instance = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss', n_jobs=-1)

    pipeline_instance = Pipeline([('classifier', classifier_instance)])

    metrics_dict, fitted_pipeline = classifier_summary(
        pipeline_instance, model_name_display,
        x_train_acc, y_train_acc, x_val_acc, y_val_acc, X_test, y_test
    )

    if metrics_dict and fitted_pipeline:
        pd.DataFrame([metrics_dict]).to_csv(os.path.join(drive_save_path, f'{model_name_file}_metrics.csv'), index=False)
        joblib.dump(fitted_pipeline, os.path.join(drive_save_path, f'{model_name_file}_model.joblib'))
        print(f"Successfully saved metrics and model for {model_name_display}.")
except Exception as e:
    print(f"An error occurred for {model_name_display}: {e}")

"""MLP Classifier"""

from sklearn.neural_network import MLPClassifier
from sklearn.pipeline import Pipeline
import pandas as pd
import joblib
import os

model_name_file = "MLP_Classifier"
model_name_display = "MLP Classifier"

print(f"\n--- Starting: {model_name_display} ---")

try:
    if not ('x_train_acc' in locals() and x_train_acc.size > 0):
        raise ValueError("x_train_acc is not defined or is empty.")

    # Instantiate the MLPClassifier.
    # early_stopping=True is highly recommended. It reserves a small part of the training data
    # as an internal validation set and stops training when the score on this set stops improving,
    # which is a great way to prevent overfitting and find a good number of epochs automatically.
    classifier_instance = MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10)

    pipeline_instance = Pipeline([('classifier', classifier_instance)])

    metrics_dict, fitted_pipeline = classifier_summary(
        pipeline_instance, model_name_display,
        x_train_acc, y_train_acc, x_val_acc, y_val_acc, X_test, y_test
    )

    if metrics_dict and fitted_pipeline:
        pd.DataFrame([metrics_dict]).to_csv(os.path.join(drive_save_path, f'{model_name_file}_metrics.csv'), index=False)
        joblib.dump(fitted_pipeline, os.path.join(drive_save_path, f'{model_name_file}_model.joblib'))
        print(f"Successfully saved metrics and model for {model_name_display}.")
except Exception as e:
    print(f"An error occurred for {model_name_display}: {e}")

"""TUNED SVM"""

# Cell: Tune the Winning SVC Model
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
import pandas as pd
import joblib
import os

model_name_file = "SVC_Tuned"
model_name_display = "SVC (Tuned)"

print(f"\n--- Starting Hyperparameter Tuning for: {model_name_display} ---")

try:
    pipeline = Pipeline([('classifier', SVC(probability=True, random_state=42))])

    # A focused grid for SVC's most important parameters
    param_grid = {
        'classifier__C': [0.1, 1, 10, 100],           # Regularization strength
        'classifier__gamma': ['scale', 'auto', 0.01, 0.1] # Kernel coefficient
    }

    grid_search = GridSearchCV(
        pipeline,
        param_grid,
        cv=3,
        scoring='accuracy',
        verbose=2,
        n_jobs=-1
    )

    print("Fitting GridSearchCV for SVC... This may take a while.")
    grid_search.fit(x_train_acc, y_train_acc) # Use the data from the treatment-grouping experiment

    print("\nBest parameters found: ", grid_search.best_params_)
    print("Best cross-validation accuracy: {:.2f}%".format(grid_search.best_score_ * 100))

    best_model_svc = grid_search.best_estimator_

    print("\n--- Evaluating Best Tuned SVC Model ---")
    metrics_dict, _ = classifier_summary(
        best_model_svc, model_name_display,
        x_train_acc, y_train_acc, x_val_acc, y_val_acc, X_test, y_test
    )

    if metrics_dict:
        # NOTE: Use a unique save path for your final tuned models!
        final_save_path = '/content/drive/MyDrive/MyPlantDiseaseModels_FinalTuned/'
        os.makedirs(final_save_path, exist_ok=True)
        pd.DataFrame([metrics_dict]).to_csv(os.path.join(final_save_path, f'{model_name_file}_metrics.csv'), index=False)
        joblib.dump(best_model_svc, os.path.join(final_save_path, f'{model_name_file}_model.joblib'))
        print(f"Successfully saved FINAL TUNED metrics and model for {model_name_display}.")

except Exception as e:
    print(f"An error occurred during SVC tuning: {e}")

"""**Cell 1: Setup and Configuration**"""

!pip install -q timm

# Cell 1: Setup and Configuration
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from PIL import Image
import timm #

import pandas as pd
import numpy as np
import os
import glob
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score
from tqdm.notebook import tqdm

# --- Configuration for this Experiment ---

# Set the device to use the GPU if available, otherwise CPU
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Choose the model architecture from the 'timm' library.
# 'efficientnet_b0' is a great starting point: strong and fast.
MODEL_NAME = 'efficientnet_b0'

# Define the image size. Models are pre-trained on specific sizes.
# 224x224 is the standard for EfficientNet-B0.
IMG_SIZE = 224

# Hyperparameters for training
BATCH_SIZE = 32         # How many images to process at once. Decrease if you run out of memory.
LEARNING_RATE = 0.001   # How big of a step the optimizer takes. 1e-3 is a common default.
EPOCHS = 5              # How many times to go through the entire training dataset.

# --- Your Class Mapping ---
# Let's use the successful treatment-based mapping (12 classes)
# This allows for a direct comparison with your best previous result.
treatment_based_mapping = {
    "wheat stripe rust": "fungal_rust", "wheat stem rust": "fungal_rust", "wheat leaf rust": "fungal_rust", "soybean rust": "fungal_rust", "raspberry yellow rust": "fungal_rust", "peach rust": "fungal_rust", "plum rust": "fungal_rust", "garlic rust": "fungal_rust", "corn rust": "fungal_rust", "coffee leaf rust": "fungal_rust", "blueberry rust": "fungal_rust", "bean rust": "fungal_rust", "apple rust": "fungal_rust",
    "wheat powdery mildew": "fungal_powdery_mildew", "zucchini powdery mildew": "fungal_powdery_mildew", "squash powdery mildew": "fungal_powdery_mildew", "cherry powdery mildew": "fungal_powdery_mildew", "cucumber powdery mildew": "fungal_powdery_mildew", "bell pepper powdery mildew": "fungal_powdery_mildew",
    "zucchini downy mildew": "fungal_downy_mildew", "soybean downy mildew": "fungal_downy_mildew", "lettuce downy mildew": "fungal_downy_mildew", "grape downy mildew": "fungal_downy_mildew", "cabbage downy mildew": "fungal_downy_mildew", "broccoli downy mildew": "fungal_downy_mildew", "basil downy mildew": "fungal_downy_mildew",
    "wheat septoria blotch": "fungal_leaf_disease", "tomato leaf mold": "fungal_leaf_disease", "tomato septoria leaf spot": "fungal_leaf_disease", "tomato late blight": "fungal_leaf_disease", "tomato early blight": "fungal_leaf_disease", "tobacco frogeye leaf spot": "fungal_leaf_disease", "soybean frog eye leaf spot": "fungal_leaf_disease", "strawberry leaf scorch": "fungal_leaf_disease", "tobacco blue mold": "fungal_leaf_disease", "tobacco brown spot": "fungal_leaf_disease", "strawberry anthracnose": "fungal_leaf_disease", "soybean brown spot": "fungal_leaf_disease", "raspberry leaf spot": "fungal_leaf_disease", "potato late blight": "fungal_leaf_disease", "rice blast": "fungal_leaf_disease", "potato early blight": "fungal_leaf_disease", "raspberry gray mold": "fungal_leaf_disease", "peach scab": "fungal_leaf_disease", "peach anthracnose": "fungal_leaf_disease", "peach leaf curl": "fungal_leaf_disease", "grape leaf spot": "fungal_leaf_disease", "ginger leaf spot": "fungal_leaf_disease", "maple tar spot": "fungal_leaf_disease", "corn northern leaf blight": "fungal_leaf_disease", "eggplant phytophthora blight": "fungal_leaf_disease", "eggplant cercospora leaf spot": "fungal_leaf_disease", "garlic leaf blight": "fungal_leaf_disease", "cherry leaf spot": "fungal_leaf_disease", "coffee brown eye spot": "fungal_leaf_disease", "corn gray leaf spot": "fungal_leaf_disease", "celery early blight": "fungal_leaf_disease", "cauliflower alternaria leaf spot": "fungal_leaf_disease", "celery anthracnose": "fungal_leaf_disease", "carrot alternaria leaf blight": "fungal_leaf_disease", "carrot cercospora leaf blight": "fungal_leaf_disease", "broccoli ring spot": "fungal_leaf_disease", "cabbage alternaria leaf spot": "fungal_leaf_disease", "blueberry botrytis blight": "fungal_leaf_disease", "bell pepper frogeye leaf spot": "fungal_leaf_disease", "broccoli alternaria leaf spot": "fungal_leaf_disease", "blueberry anthracnose": "fungal_leaf_disease", "blueberry scorch": "fungal_leaf_disease", "banana panama disease": "fungal_leaf_disease", "banana black leaf streak": "fungal_leaf_disease", "banana cordana leaf spot": "fungal_leaf_disease", "banana anthracnose": "fungal_leaf_disease", "apple scab": "fungal_leaf_disease", "rice sheath blight": "fungal_leaf_disease", "ginger sheath blight": "fungal_leaf_disease",
    "peach brown rot": "fungal_rot_fruit_disease", "plum brown rot": "fungal_rot_fruit_disease", "grape black rot": "fungal_rot_fruit_disease", "eggplant phomopsis fruit rot": "fungal_rot_fruit_disease", "coffee black rot": "fungal_rot_fruit_disease", "blueberry mummy berry": "fungal_rot_fruit_disease", "banana cigar end rot": "fungal_rot_fruit_disease", "apple black rot": "fungal_rot_fruit_disease",
    "bell pepper blossom end rot": "abiotic_disorder",
    "wheat loose smut": "fungal_systemic_smut_gall", "corn smut": "fungal_systemic_smut_gall", "plum pocket disease": "fungal_systemic_smut_gall",
    "zucchini bacterial wilt": "bacterial_disease", "tomato bacterial leaf spot": "bacterial_disease", "wheat bacterial leaf streak (black chaff)": "bacterial_disease", "soybean bacterial blight": "bacterial_disease", "plum bacterial spot": "bacterial_disease", "cucumber angular leaf spot": "bacterial_disease", "cucumber bacterial wilt": "bacterial_disease", "citrus canker": "bacterial_disease", "citrus greening disease": "bacterial_disease", "cabbage black rot": "bacterial_disease", "cauliflower bacterial soft rot": "bacterial_disease", "bean halo blight": "bacterial_disease", "bell pepper bacterial spot": "bacterial_disease", "raspberry fire blight": "bacterial_disease",
    "zucchini yellow mosaic virus": "viral_disease", "tomato mosaic virus": "viral_disease", "tomato yellow leaf curl virus": "viral_disease", "tobacco mosaic virus": "viral_disease", "soybean mosaic": "viral_disease", "plum pox virus": "viral_disease", "lettuce mosaic virus": "viral_disease", "grapevine leafroll disease": "viral_disease", "banana bunchy top": "viral_disease", "bean mosaic virus": "viral_disease", "apple mosaic virus": "viral_disease",
    "carrot cavity spot": "oomycete_lesion", "coffee berry blotch": "fungal_rot_fruit_disease",
    "wheat head scab": "fungal_scab",
}

print(f"Setup Complete. Using device: {DEVICE}")

"""**Cell 2: Data Preparation (Create Datasets and DataLoaders)**"""

# Corrected Cell 2: Data Preparation (Create Datasets and DataLoaders)
import os
# --- 1. Find all image paths and create a master DataFrame ---
base_path = '/content/drive/MyDrive/plantwild_split'
all_image_paths = glob.glob(os.path.join(base_path, '*/*/*.jpg'))
path_df = pd.DataFrame(all_image_paths, columns=['image_path'])
path_df['original_label'] = path_df['image_path'].apply(lambda x: os.path.basename(os.path.dirname(x)))
path_df['split'] = path_df['image_path'].apply(lambda x: os.path.basename(os.path.dirname(os.path.dirname(x))))
print(f"Found {len(path_df)} total images.")

# --- 2. Apply class mapping and encode the new grouped labels ---
path_df['grouped_label'] = path_df['original_label'].map(treatment_based_mapping)
path_df = path_df.dropna(subset=['grouped_label'])

final_encoder = LabelEncoder()
path_df['encoded_label'] = final_encoder.fit_transform(path_df['grouped_label'])
NUM_CLASSES = len(final_encoder.classes_)
print(f"\nApplied mapping. Found {NUM_CLASSES} unique grouped classes.")
print("Class mapping (number -> name):")
for i, class_name in enumerate(final_encoder.classes_):
    print(f"{i}: {class_name}")

# --- 3. Split the DataFrame into train, validation, and test sets ---
train_full_df = path_df[path_df['split'] == 'train'].reset_index(drop=True)
test_df_original = path_df[path_df['split'] == 'test'] # No need to reset yet

# Create a validation set from the training set (stratified)
train_df_original, val_df_original = train_test_split(train_full_df, test_size=0.2, random_state=42, stratify=train_full_df['encoded_label'])

# --- THE FIX IS HERE: Reset the index of the final DataFrames ---
train_df = train_df_original.reset_index(drop=True)
val_df = val_df_original.reset_index(drop=True)
test_df = test_df_original.reset_index(drop=True)
# The 'drop=True' argument prevents pandas from adding the old index as a new column.

print(f"\nData split sizes (after resetting index):")
print(f"Training set: {len(train_df)} samples")
print(f"Validation set: {len(val_df)} samples")
print(f"Test set: {len(test_df)} samples")

# --- 4. Define the custom PyTorch Dataset class ---
# This class definition does not need to change
class PlantDiseaseDataset(Dataset):
    def __init__(self, df, transform=None):
        self.df = df
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        # NOW, using .loc[idx] is safe because the index has been reset.
        img_path = self.df.loc[idx, 'image_path']
        label = self.df.loc[idx, 'encoded_label']
        image = Image.open(img_path).convert("RGB")
        if self.transform:
            image = self.transform(image)
        return image, torch.tensor(label, dtype=torch.long)

# --- 5. Create image transformations using 'timm' ---
# This part does not need to change
model_for_transforms = timm.create_model(MODEL_NAME, pretrained=True)
data_config = timm.data.resolve_data_config({}, model=model_for_transforms)
transform = timm.data.create_transform(**data_config)
print("\nImage transforms created.")

# --- 6. Create the final Dataset and DataLoader objects ---
# This part does not need to change
train_dataset = PlantDiseaseDataset(train_df, transform=transform)
val_dataset = PlantDiseaseDataset(val_df, transform=transform)
test_dataset = PlantDiseaseDataset(test_df, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)

print("\nDataLoaders created and ready for training.")
print(f"Number of batches per epoch: Train={len(train_loader)}, Validation={len(val_loader)}, Test={len(test_loader)}")

"""**Cell 3: Model Fine-Tuning Loop**"""

# Cell 3: Model Fine-Tuning Loop

# --- 1. Instantiate the Model ---
# Create the EfficientNet-B0 model, pre-trained on ImageNet.
# The final layer (classifier) is automatically replaced with a new one
# that has 'NUM_CLASSES' outputs.
print(f"Loading pre-trained model: {MODEL_NAME}")
model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=NUM_CLASSES)
model.to(DEVICE) # Move the model to the GPU

# --- 2. Define Loss Function and Optimizer ---
criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)

# --- 3. Training and Validation Loop ---
best_val_accuracy = 0.0
model_save_path = f"/content/drive/MyDrive/{MODEL_NAME}_best_model.pth" # Path to save the best model

print("\n--- Starting Model Fine-Tuning ---")

for epoch in range(EPOCHS):
    # --- Training Phase ---
    model.train()
    running_loss = 0.0

    # Use tqdm for a progress bar over the training batches
    for inputs, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [Training]"):
        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        # Backward pass and optimize
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * inputs.size(0)

    epoch_loss = running_loss / len(train_dataset)

    # --- Validation Phase ---
    model.eval() # Set the model to evaluation mode
    val_preds = []
    val_labels = []

    # We don't need to track gradients during validation, which saves memory and computations
    with torch.no_grad():
        for inputs, labels in tqdm(val_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [Validation]"):
            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)

            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1) # Get the index of the max log-probability

            val_preds.extend(predicted.cpu().numpy())
            val_labels.extend(labels.cpu().numpy())

    val_accuracy = accuracy_score(val_labels, val_preds)

    print(f"Epoch {epoch+1}/{EPOCHS} | Train Loss: {epoch_loss:.4f} | Val Accuracy: {val_accuracy:.4f}")

    # Save the model checkpoint if it's the best one we've seen so far
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), model_save_path)
        print(f"--> New best model saved to {model_save_path} with validation accuracy: {best_val_accuracy:.4f}")

print("\nFinished Training.")

# --- Resume Training from Checkpoint ---

import torch
import timm
from torch import nn, optim
from sklearn.metrics import accuracy_score
from tqdm import tqdm

# --- Settings ---
MODEL_NAME = "efficientnet_b0"
NUM_CLASSES = 11
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_save_path = "/content/drive/MyDrive/efficientnet_b0_best_model.pth"
resume_epochs = 10  # How many additional epochs to train
new_lr = 5e-5        # Lower LR for fine-tuning

# --- Reload Model Architecture and Weights ---
model = timm.create_model(MODEL_NAME, pretrained=False, num_classes=NUM_CLASSES)
model.load_state_dict(torch.load(model_save_path))
model.to(DEVICE)

# --- Define Loss and Optimizer ---
criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=new_lr)

# --- Resume Training Loop ---
print(f"\nResuming training for {resume_epochs} more epochs...")

for epoch in range(resume_epochs):
    model.train()
    running_loss = 0.0

    for inputs, labels in tqdm(train_loader, desc=f"Resumed Epoch {epoch+1}/{resume_epochs} [Training]"):
        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * inputs.size(0)

    epoch_loss = running_loss / len(train_dataset)

    # --- Validation ---
    model.eval()
    val_preds, val_labels = [], []

    with torch.no_grad():
        for inputs, labels in tqdm(val_loader, desc=f"Resumed Epoch {epoch+1}/{resume_epochs} [Validation]"):
            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            val_preds.extend(predicted.cpu().numpy())
            val_labels.extend(labels.cpu().numpy())

    val_accuracy = accuracy_score(val_labels, val_preds)

    print(f"Epoch {epoch+1}/{resume_epochs} | Train Loss: {epoch_loss:.4f} | Val Accuracy: {val_accuracy:.4f}")

    # Optional: save best again
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), model_save_path)
        print(f"--> Updated best model saved with val acc: {best_val_accuracy:.4f}")

print(" Finished resumed training.")